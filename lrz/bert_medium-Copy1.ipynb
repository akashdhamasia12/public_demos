{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fed133d-61b7-4ce6-8a44-fe98acf0eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3a8fd1-25a9-426d-a6be-c93b750cbcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a53036-31ab-4374-bf15-a4dca17a7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = f'bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab965eff-e1eb-416f-b80c-850554d8026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['category']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5074c270-ed3e-4e1a-863d-71737c743cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 501kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 4.96kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 183kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'business':0,\n",
    "          'entertainment':1,\n",
    "          'sport':2,\n",
    "          'tech':3,\n",
    "          'politics':4\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8a5d0f-80c3-42b3-9f06-ecfc3a21f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa1f1cf7-65db-4966-9a55-ba26bd22ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs, criterion, optimizer, IPEX):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if IPEX:\n",
    "        device = \"xpu\"\n",
    "\n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "            \n",
    "    duration = []\n",
    "    duration_model = []\n",
    "    t1 = time.time()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                t2 = time.time()\n",
    "                output = model(input_id, mask)\n",
    "                dur_model = time.time() - t2\n",
    "                \n",
    "                duration_model.append(dur_model)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            test_time = time.time() - t1\n",
    "            \n",
    "            duration.append(test_time)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "\n",
    "#                 for val_input, val_label in val_dataloader:\n",
    "\n",
    "#                     val_label = val_label.to(device)\n",
    "#                     mask = val_input['attention_mask'].to(device)\n",
    "#                     input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "#                     output = model(input_id, mask)\n",
    "\n",
    "#                     batch_loss = criterion(output, val_label.long())\n",
    "#                     total_loss_val += batch_loss.item()\n",
    "                    \n",
    "#                     acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "#                     total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "    \n",
    "    return duration, np.average(duration_model)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8a670d-c449-45fe-8f4c-9a5fb27855c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, IPEX):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if IPEX:\n",
    "        device = \"xpu\"\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        duration_inf = []\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            t1 = time.time()\n",
    "            output = model(input_id, mask)\n",
    "            dur = time.time() - t1\n",
    "            \n",
    "            duration_inf.append(dur)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return np.average(duration_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d2231d-fef1-42cf-a73e-188cac932727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 222 223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30242239-de70-4c03-8f56-9f5ade43518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|███████████████████████████████████████████████████████████████| 436M/436M [00:09<00:00, 44.7MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [07:16<00:00,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.051 | Train Accuracy:  0.219 | Val Loss:  0.000 | Val Accuracy:  0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IPEX = False\n",
    "EPOCHS = 1\n",
    "model_org = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "model_org.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_org.parameters(), lr=LR)\n",
    "\n",
    "duration, duration_model = train(model_org, df_train, df_val, LR, EPOCHS, criterion, optimizer, IPEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc00f0a-9a15-4942-9c9b-2f9789c8dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.251\n"
     ]
    }
   ],
   "source": [
    "IPEX = False\n",
    "model_org.eval()\n",
    "duration_inf = evaluate(model_org, df_test, IPEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "582d08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:447: UserWarning: For XPU device, the split master weight is unsupported for now, so temp to disable it\n",
      "  warnings.warn(\"For XPU device, the split master weight is unsupported for now, so temp to disable it\")\n",
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:457: UserWarning: For XPU device to save valuable device memory, temp to do optimization on inplaced model, so                     make inplace to be true\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:464: UserWarning: For XPU, the weight prepack and sample input are disabled. The onednn layout                     is automatically chosen to use\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/optim/_optimizer_utils.py:250: UserWarning: Does not suport fused step for <class 'torch.optim.adam.Adam'>, will use non-fused step\n",
      "  warnings.warn(\"Does not suport fused step for \" + str(type(optimizer)) + \", will use non-fused step\")\n",
      "  0%|                                                                                                                | 0/56 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Native API failed. Native API returns: -997 (The plugin has emitted a backend specific error) -997 (The plugin has emitted a backend specific error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model_train, optimizer \u001b[38;5;241m=\u001b[39m ipex\u001b[38;5;241m.\u001b[39moptimize(model, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# optimizer = Adam(model_train.parameters(), lr=LR)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m duration_ipex, duration_model_ipex \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIPEX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs, criterion, optimizer, IPEX)\u001b[0m\n\u001b[1;32m     31\u001b[0m input_id \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m dur_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t2\n\u001b[1;32m     37\u001b[0m duration_model\u001b[38;5;241m.\u001b[39mappend(dur_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_id, mask):\n\u001b[0;32m---> 14\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     dropout_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m     16\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dropout_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1012\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1013\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1014\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[0;32m-> 1019\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1032\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    600\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:350\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Native API failed. Native API returns: -997 (The plugin has emitted a backend specific error) -997 (The plugin has emitted a backend specific error)"
     ]
    }
   ],
   "source": [
    "#with IPEX\n",
    "IPEX = True\n",
    "import intel_extension_for_pytorch as ipex\n",
    "model = BertClassifier()              \n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "model.train()\n",
    "model = model.to(\"xpu\")\n",
    "criterion = criterion.to(\"xpu\")\n",
    "model_train, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "# optimizer = Adam(model_train.parameters(), lr=LR)\n",
    "duration_ipex, duration_model_ipex = train(model_train, df_train, df_val, LR, EPOCHS,criterion, optimizer, IPEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e449263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:486: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n",
      "/home/gta/miniconda3/envs/bert_profile_xpu/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:491: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Linear BatchNorm folding failed during the optimize process.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.251\n"
     ]
    }
   ],
   "source": [
    "#with IPEX\n",
    "IPEX = True\n",
    "model_org.eval()\n",
    "model_org = model_org.to(\"xpu\")\n",
    "model_ipex = ipex.optimize(model_org)\n",
    "duration_inf_ipex = evaluate(model_ipex, df_test, IPEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96c959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.251\n"
     ]
    }
   ],
   "source": [
    "#with IPEX bfloat16\n",
    "IPEX = True\n",
    "model.eval()\n",
    "model_ipex_bf16 = ipex.optimize(model_org, dtype=torch.bfloat16)\n",
    "duration_inf_ipex_bf16 = evaluate(model_ipex_bf16, df_test, IPEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1e0af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'duration_model_ipex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m      4\u001b[0m time_training_model \u001b[38;5;241m=\u001b[39m duration_model\n\u001b[0;32m----> 5\u001b[0m time_training_model_ipex \u001b[38;5;241m=\u001b[39m \u001b[43mduration_model_ipex\u001b[49m\n\u001b[1;32m      7\u001b[0m pred_times \u001b[38;5;241m=\u001b[39m [time_training_model, time_training_model_ipex]\n\u001b[1;32m      8\u001b[0m tick_label \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStock PyTorch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith IPEX Float32\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'duration_model_ipex' is not defined"
     ]
    }
   ],
   "source": [
    "#show performanace boost: Training (of only model)\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "time_training_model = duration_model\n",
    "time_training_model_ipex = duration_model_ipex\n",
    "\n",
    "pred_times = [time_training_model, time_training_model_ipex]\n",
    "tick_label = ['Stock PyTorch', 'with IPEX Float32']\n",
    "\n",
    "left = [1,2]\n",
    "plt.bar(left, pred_times, tick_label = tick_label, width = 0.5, color = ['blue', 'red'])\n",
    "plt.xlabel('Test Method'); plt.ylabel('time,s'); plt.title('IPEX Training Time,s')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# print(\"Performance Speedup Training: \",(time_train)/(time_train_ipex))\n",
    "print(\"Performance Speedup Training model: \",(time_training_model)/(time_training_model_ipex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf448ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #show performanace boost: Training (epoch by epoch)\n",
    "  \n",
    "# X = ['Epoch1','Epoch2','Epoch3','Epoch4','Epoch5']\n",
    "  \n",
    "# X_axis = np.arange(len(X))\n",
    "  \n",
    "# plt.bar(X_axis - 0.2, duration, 0.4, label = 'Stock PyTorch')\n",
    "# plt.bar(X_axis + 0.2, duration_ipex, 0.4, label = 'IPEX')\n",
    "  \n",
    "# plt.xticks(X_axis, X)\n",
    "# plt.xlabel(\"Groups\")\n",
    "# plt.ylabel(\"Latency\")\n",
    "# plt.title(\"Epoch by Epoch comparison\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # print(\"Performance Speedup Training: \",(time_train)/(time_train_ipex))\n",
    "# print(\"Performance Speedup Training model: \",(np.average(duration))/(np.average(duration_ipex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "040fe17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Speedup testing model:  2.602841608272591\n"
     ]
    }
   ],
   "source": [
    "#show performanace boost: Inference\n",
    "\n",
    "time_training_model = duration_inf\n",
    "time_training_model_ipex = duration_inf_ipex\n",
    "time_training_model_ipex_bf16 = duration_inf_ipex_bf16\n",
    "\n",
    "pred_times = [time_training_model, time_training_model_ipex, time_training_model_ipex_bf16]\n",
    "tick_label = ['Stock PyTorch', 'with IPEX Float32', 'with IPEX bfloat16']\n",
    "\n",
    "left = [1,2,3]\n",
    "plt.bar(left, pred_times, tick_label = tick_label, width = 0.5, color = ['blue', 'red', 'orange'])\n",
    "plt.xlabel('Test Method'); plt.ylabel('time,s'); plt.title('IPEX Testing Time,s')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# print(\"Performance Speedup Training: \",(time_train)/(time_train_ipex))\n",
    "print(\"Performance Speedup testing model: \",(time_training_model)/(time_training_model_ipex_bf16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782859dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bab8aa6ddb43f43a624cc9a0e701ed05dcf01833365ee0175951444b8b2898e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
