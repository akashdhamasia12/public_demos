{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d92e7-10aa-44db-8957-e25901a96792",
   "metadata": {},
   "source": [
    "## Stable Diffusion on SPR with IPEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebf84a4-42f9-4c8d-ae63-f209cac19a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W OperatorEntry.cpp:150] Warning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: torchvision::nms\n",
      "    no debug info\n",
      "  dispatch key: CPU\n",
      "  previous kernel: registered at /build/intel-pytorch-extension/csrc/cpu/aten/TorchVisionNms.cpp:47\n",
      "       new kernel: registered at /opt/workspace/vision/torchvision/csrc/ops/cpu/nms_kernel.cpp:112 (function registerKernel)\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba43447-9020-416a-a22f-88d9ed50fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intel token\n",
    "# MY_TOKEN=\"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\"\n",
    "# My token\n",
    "MY_TOKEN='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7f741b-bc0e-4d5b-a038-a8118257890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Load models and create wrapper for stable diffusion\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\", use_auth_token=MY_TOKEN)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_auth_token=MY_TOKEN)\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=MY_TOKEN)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=MY_TOKEN)\n",
    "\n",
    "unet.eval()\n",
    "unet = unet.to('xpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a11ec4-f683-4668-87d4-19db9ba0b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:447: UserWarning: For XPU device, the split master weight is unsupported for now, so temp to disable it\n",
      "  warnings.warn(\"For XPU device, the split master weight is unsupported for now, so temp to disable it\")\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:457: UserWarning: For XPU device to save valuable device memory, temp to do optimization on inplaced model, so                     make inplace to be true\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:464: UserWarning: For XPU, the weight prepack and sample input are disabled. The onednn layout                     is automatically chosen to use\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:486: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:491: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Linear BatchNorm folding failed during the optimize process.\")\n"
     ]
    }
   ],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# unet.eval()\n",
    "# text_encoder.eval()\n",
    "# unet = unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "unet = ipex.optimize(unet)\n",
    "\n",
    "# unet = ipex.optimize(unet, dtype=torch.bfloat16)\n",
    "# text_encoder = ipex.optimize(text_encoder, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68275cc2-9cd2-4f18-8766-04baf19ada5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:100: FutureWarning: The configuration file of this scheduler: PNDMScheduler {\n",
      "  \"_class_name\": \"PNDMScheduler\",\n",
      "  \"_diffusers_version\": \"0.12.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"skip_prk_steps\": true,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline(\n",
    "    text_encoder=text_encoder,\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=PNDMScheduler(beta_start=0.00085, \n",
    "                            beta_end=0.012, \n",
    "                            beta_schedule=\"scaled_linear\", \n",
    "                            skip_prk_steps=True),\n",
    "    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "    feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")).to('xpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b1f7ca-facb-44a6-8761-c206c4c5dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:07<00:00,  6.52it/s]\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"\n",
    "\n",
    "# Setting seed for deterministic output\n",
    "# generator = torch.Generator(\"xpu\").manual_seed(777)\n",
    "\n",
    "# with torch.xpu.amp.autocast():\n",
    "image = pipeline(prompt, num_inference_steps=50).images[0]\n",
    "# image = pipeline(prompt, num_inference_steps=50, generator=generator).images[0]\n",
    "\n",
    "image.save(\"frog_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb03b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "prompt = \"frog on the bike\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', torch_dtype=torch.float32)\n",
    "import intel_extension_for_pytorch\n",
    "pipe = pipe.to(\"xpu\")\n",
    "image = pipe(prompt).images[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f7f53-4265-4d37-9c5d-68437af06480",
   "metadata": {},
   "source": [
    "**Batched inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67863559-8a47-456e-bf58-27037c7a01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f235e0-7698-49b3-9bd7-7772bf001283",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 8\n",
    "\n",
    "prompt = [\"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"] * num_images\n",
    "\n",
    "# with torch.xpu.amp.autocast():\n",
    "images = pipeline(prompt).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=3)\n",
    "\n",
    "grid.save(f\"frog_batch.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
